# Knowledge Base Crawler

A flexible web crawler that saves pages locally and converts them to various formats.

## Features

- ğŸŒ Crawls web pages within the same domain
- ğŸ“ Local caching of HTML content
- ğŸ” Flexible URL pattern matching using micromatch
- ğŸ“ Multiple output formats (raw HTML, Markdown, structured knowledge)
- ğŸ¯ CSS selector support for content extraction

## Installation

```bash
pnpm install
```

## Usage

```bash
pnpm dev -e <entrypoint-url> [options]
```

### Options

- `-e, --entrypoint <url>` - Starting URL (required)
- `-p, --pattern <pattern>` - URL pattern to match (default: "**/*")
- `-s, --selector <selector>` - HTML selector (default: "body")
- `-f, --format <format>` - Output format: raw/markdown/knowledge (default: "raw")

### Examples

```bash
# Crawl all docs pages
pnpm dev -e https://example.com/docs/start -p "/docs/**" -s "main" -f markdown

# Crawl specific sections
pnpm dev -e https://example.com -p "/{docs,blog}/**" -s "article" -f knowledge

# Exclude certain paths
pnpm dev -e https://example.com -p "/docs/!(temp)/**" -s "main" -f raw
```

## Cache Structure

```
cache/
â”œâ”€â”€ pages/              # Cached HTML files
â”‚   â””â”€â”€ {domain}/      # Organized by domain
â”‚       â””â”€â”€ {hash}.html
â””â”€â”€ metadata/          # Page metadata
    â””â”€â”€ {domain}/
        â””â”€â”€ {hash}.json
```

## Development

```bash
# Run tests
pnpm test

# Format code
pnpm format
```

## License

ISC 
